{
  
    
        "post0": {
            "title": "Title",
            "content": "OpenML is an online Machine Learning (ML) experiments database accessible to everyone for free. The core idea is to have a single repository of datasets and results of ML experiments on them. Despite having gained a lot of popularity in recent years, with a plethora of tools now available, the numerous ML experimentations continue to happen in silos and not necessarily as one whole shared community. In this post, we shall try to get a brief glimpse of what OpenML offers and how it can fit our current Machine Learning practices. . Let us jump straight at getting our hands dirty by building a simple machine learning model. If it is simplicity we are looking for, it has to be the Iris dataset that we shall work with. In the example script below, we are going to load the Iris dataset available with scikit-learn, use 10-fold cross-validation to evaluate a Random Forest of 10 trees. Sounds trivial enough and is indeed less than 10 lines of code. . from sklearn import datasets from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score . # Loading Iris dataset X, y = datasets.load_iris(return_X_y=True) print(X.shape, y.shape) . (150, 4) (150,) . # Initializing a Random Forest with # arbitrary hyperparameters # max_depth kept as 2 since Iris has # only 4 features clf = RandomForestClassifier(n_estimators=10, max_depth=2) . scores = cross_val_score(clf, X, y, cv=5, scoring=&#39;accuracy&#39;) print(&quot;Mean score : {:.5f}&quot;.format(scores.mean())) . Mean score : 0.94000 . A simple script and we achieve a mean accuracy of 95.33%. That was easy. It is really amazing how far we have come with ML tools that make it easy to get started. As a result, we have hundreds of thousands of people working with these tools every day. That inevitably leads to the reinvention of the wheel. The tasks that each individual ML practitioner performs often have significant overlaps and can be omitted by reusing what someone from the community has done already. At the end of the day, we didn&#39;t build a Random Forest model all the way from scratch. We gladly reused code written by generous folks from the community. The special attribute of our species is the ability to work as a collective wherein our combined intellect becomes larger than the individual sum of parts. Why not do the same for ML? I mean, can I see what other ML practitioners have done to get better scores on the Iris dataset? . Answering this is one of the targets of this post. We shall subsequently explore if this can be done, with the help of OpenML. However, first, we shall briefly familiarize ourselves with few terminologies and see how we can split the earlier example we saw into modular components. . OpenML Components . Image source: https://medium.com/open-machine-learning/openml-1e0d43f0ae13 . Dataset: OpenML houses over 2k+ active datasets for various regression, classification, clustering, survival analysis, stream processing tasks and more. Any user can upload a dataset. Once uploaded, the server computes certain meta-features on the dataset - Number of classes, Number of missing values, Number of features, etc. With respect to our earlier example, the following line is the equivalent of fetching a dataset from OpenML. . X, y = datasets.load_iris(return_X_y=True) . Task: A task is linked to a specific dataset, defining what the target/dependent variable is. Also specifies evaluation measures such as - accuracy, precision, area under curve, etc. or the kind of estimation procedure to be used such as - 10-fold cross-validation, n% holdout set, etc. With respect to our earlier example, the parameters to the following function call capture the idea of a task. . scores = cross_val_score(clf, X, y, cv=5, scoring=&#39;accuracy&#39;) . Flow: Describes the kind of modelling to be performed. It could be a flow or a series of steps, i.e., a scikit-learn pipeline. For now, we have used a simple Random Forest model which is the flow component here. . clf = RandomForestClassifier(n_estimators=10, max_depth=2) . Run: Pairs a flow and task together which results in a run. The run has the predictions which are turned into evaluations by the server. This is effectively captured by the execution of the line: . scores = cross_val_score(clf, X, y, cv=5, scoring=&#39;accuracy&#39;) . Now, this may appear a little obfuscating given that we are trying to compartmentalize a simple 10-line code which works just fine. However, if we take a few seconds to go through the 4 components explained above, we can see that it makes our training of a Random Forest on Iris a series of modular tasks. Modules are such a fundamental concept in Computer Science. They are like Lego blocks. Once we have modules, it means we can plug and play at ease. The code snippet below attempts to rewrite the earlier example using the ideas of the OpenML components described, to give a glimpse of what we can potentially gain during experimentations. . from sklearn import datasets from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score . DATASET component . # To load IRIS dataset as a dataset module/component def dataset(): X, y = datasets.load_iris(return_X_y=True) return X, y . TASK component . # Tasks here define the number of cross-validation folds # and the scoring metric to be used for evaluation def task_1(f): X, y = dataset() # loads IRIS return cross_val_score(f, X, y, cv=5, scoring=&#39;accuracy&#39;) def task_2(f): X, y = dataset() # loads IRIS return cross_val_score(f, X, y, cv=15, scoring=&#39;balanced_accuracy&#39;) . FLOW component . # Flows determine the modelling technique to be applied # Helps define a model irrespective of dataset or tasks def flow_1(): clf = RandomForestClassifier(n_estimators=10, max_depth=2) return clf def flow_2(): clf = SVC(gamma=&#39;auto&#39;, kernel=&#39;linear&#39;) return clf . RUN component . # Runs essentially evaluates a task-flow pairing # and therefore in effect executs the modelling # of a dataset as per the task task definition def run(task, flow): return task(flow) . # Results for Random Forest rf_task_1 = run(task_1, flow_1()) rf_task_2 = run(task_2, flow_1()) print(&quot;RF using task 1: {:&lt;.5}; task 2: {:&lt;.5}&quot;.format(rf_task_1.mean(), rf_task_2.mean())) # Results for SVM svm_task_1 = run(task_1, flow_2()) svm_task_2 = run(task_2, flow_2()) print(&quot;SVM using task 1: {:&lt;.5}; task 2: {:&lt;.5}&quot;.format(svm_task_1.mean(), svm_task_2.mean())) . RF using task 1: 0.95333; task 2: 0.94444 SVM using task 1: 0.98; task 2: 0.97222 . We can, therefore, compose various different tasks, flows, which are independent operations. Runs can then pair any such task and flow to construct an ML workflow and return the evaluated scores. This approach can help us define such components one-time, and we can extend this for any combination of a dataset, model, and for any number of evaluations in the future. Imagine if the entire ML community defines such tasks and various simple to complicated flows that they use in their daily practice. We can build custom working ML pipeline and even get to compare performances of our techniques on the same task with others! OpenML aims exactly for that. In the next section of this post, we shall scratch the surface of OpenML to see if we can actually do with OpenML what it promises. . Using OpenML . OpenML-Python can be installed using pip or by cloning the git repo and installing the current development version. So shall we then install OpenML? ;) It will be beneficial if the code snippets are tried out as this post is read. A consolidated Jupyter notebook with all the code can be found here. . Now that we have OpenML, let us jump straight into figuring out how we can get the Iris dataset from there. We can always browse theOpenML website and search for Iris. That is the easy route. Let us get familiar with the programmatic approach and learn how to fish instead. The OpenML-Python API can be found here. . Retrieving Iris from&#160;OpenML . In the example below, we will list out all possible datasets available in OpenML. We can choose the output format. I&#39;ll go with dataframe so that we obtain a pandas DataFrame and can get a neat tabular representation to search and sort specific entries. . import openml import numpy as np import pandas as pd . # Fetching the list of all available datasets on OpenML d = openml.datasets.list_datasets(output_format=&#39;dataframe&#39;) print(d.shape) # Listing column names or attributes that OpenML offers for name in d.columns: print(name) . (3073, 16) did name version uploader status format MajorityClassSize MaxNominalAttDistinctValues MinorityClassSize NumberOfClasses NumberOfFeatures NumberOfInstances NumberOfInstancesWithMissingValues NumberOfMissingValues NumberOfNumericFeatures NumberOfSymbolicFeatures . print(d.head()) . did name version uploader status format MajorityClassSize 2 2 anneal 1 1 active ARFF 684.0 3 3 kr-vs-kp 1 1 active ARFF 1669.0 4 4 labor 1 1 active ARFF 37.0 5 5 arrhythmia 1 1 active ARFF 245.0 6 6 letter 1 1 active ARFF 813.0 MaxNominalAttDistinctValues MinorityClassSize NumberOfClasses 2 7.0 8.0 5.0 3 3.0 1527.0 2.0 4 3.0 20.0 2.0 5 13.0 2.0 13.0 6 26.0 734.0 26.0 NumberOfFeatures NumberOfInstances NumberOfInstancesWithMissingValues 2 39.0 898.0 898.0 3 37.0 3196.0 0.0 4 17.0 57.0 56.0 5 280.0 452.0 384.0 6 17.0 20000.0 0.0 NumberOfMissingValues NumberOfNumericFeatures NumberOfSymbolicFeatures 2 22175.0 6.0 33.0 3 0.0 0.0 37.0 4 326.0 8.0 9.0 5 408.0 206.0 74.0 6 0.0 16.0 1.0 . The column names indicate that they contain the meta-information about each of the datasets, and at this instance, we have access to 2958 datasets as indicated by the shape of the dataframe. We shall try searching for &#39;iris&#39; in the column name and also use the version column to sort the results. . # Filtering dataset list to have &#39;iris&#39; in the &#39;name&#39; column # then sorting the list based on the &#39;version&#39; d[d[&#39;name&#39;].str.contains(&#39;iris&#39;)].sort_values(by=&#39;version&#39;).head() . did name version uploader status format MajorityClassSize MaxNominalAttDistinctValues MinorityClassSize NumberOfClasses NumberOfFeatures NumberOfInstances NumberOfInstancesWithMissingValues NumberOfMissingValues NumberOfNumericFeatures NumberOfSymbolicFeatures . 61 61 | iris | 1 | 1 | active | ARFF | 50.0 | 3.0 | 50.0 | 3.0 | 5.0 | 150.0 | 0.0 | 0.0 | 4.0 | 1.0 | . 41950 41950 | iris_test_upload | 1 | 4030 | active | ARFF | 50.0 | 3.0 | 50.0 | 3.0 | 5.0 | 150.0 | 0.0 | 0.0 | 4.0 | 1.0 | . 42261 42261 | iris-example | 1 | 348 | active | ARFF | 50.0 | NaN | 50.0 | 3.0 | 5.0 | 150.0 | 0.0 | 0.0 | 4.0 | 1.0 | . 451 451 | irish | 1 | 2 | active | ARFF | 278.0 | 10.0 | 222.0 | 2.0 | 6.0 | 500.0 | 32.0 | 32.0 | 2.0 | 4.0 | . 969 969 | iris | 3 | 2 | active | ARFF | 100.0 | 2.0 | 50.0 | 2.0 | 5.0 | 150.0 | 0.0 | 0.0 | 4.0 | 1.0 | . Okay, so the iris dataset with the version as 1 has an ID of 61. For verification, we can check the website for dataset ID 61. We can see that it is the original Iris dataset which is of interest to us - 3 classes of 50 instances, with 4 numeric features. However, we shall retrieve the same information, as promised, programmatically. . iris = openml.datasets.get_dataset(61) iris . OpenML Dataset ============== Name..........: iris Version.......: 1 Format........: ARFF Upload Date...: 2014-04-06 23:23:39 Licence.......: Public Download URL..: https://www.openml.org/data/v1/download/61/iris.arff OpenML URL....: https://www.openml.org/d/61 # of features.: 5 # of instances: 150 . iris.features . {0: [0 - sepallength (numeric)], 1: [1 - sepalwidth (numeric)], 2: [2 - petallength (numeric)], 3: [3 - petalwidth (numeric)], 4: [4 - class (nominal)]} . print(iris.description) . **Author**: R.A. Fisher **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall **Please cite**: **Iris Plants Database** This is perhaps the best known database to be found in the pattern recognition literature. Fisher&#39;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. Predicted attribute: class of iris plant. This is an exceedingly simple domain. ### Attribute Information: 1. sepal length in cm 2. sepal width in cm 3. petal length in cm 4. petal width in cm 5. class: -- Iris Setosa -- Iris Versicolour -- Iris Virginica . With the appropriate dataset available, let us briefly go back to the terminologies we discussed earlier. We have only used the dataset component so far. The dataset component is closely tied with the task component. To reiterate, the task would describe how the dataset will be used. . Retrieving relevant tasks from&#160;OpenML . We shall firstly list all available tasks that work with the Iris dataset. However, we are only treating Iris as a supervised classification problem and hence will filter accordingly. Following which, we will collect only the task IDs of the tasks relevant to us. . df = openml.tasks.list_tasks(data_id=61, output_format=&#39;dataframe&#39;) df.head() . tid ttid did name task_type status estimation_procedure evaluation_measures source_data target_feature ... NumberOfFeatures NumberOfInstances NumberOfInstancesWithMissingValues NumberOfMissingValues NumberOfNumericFeatures NumberOfSymbolicFeatures number_samples cost_matrix quality_measure target_value . 59 59 | 1 | 61 | iris | Supervised Classification | active | 10-fold Crossvalidation | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | NaN | NaN | NaN | NaN | . 118 118 | 3 | 61 | iris | Learning Curve | active | 10 times 10-fold Learning Curve | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | 4 | NaN | NaN | NaN | . 289 289 | 1 | 61 | iris | Supervised Classification | active | 33% Holdout set | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | NaN | NaN | NaN | NaN | . 1758 1758 | 3 | 61 | iris | Learning Curve | active | 10-fold Learning Curve | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | 4 | NaN | NaN | NaN | . 1823 1823 | 1 | 61 | iris | Supervised Classification | active | 5 times 2-fold Crossvalidation | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | NaN | NaN | NaN | NaN | . 5 rows × 24 columns . # Filtering only the Supervised Classification tasks on Iris df.query(&quot;task_type==&#39;Supervised Classification&#39;&quot;).head() . tid ttid did name task_type status estimation_procedure evaluation_measures source_data target_feature ... NumberOfFeatures NumberOfInstances NumberOfInstancesWithMissingValues NumberOfMissingValues NumberOfNumericFeatures NumberOfSymbolicFeatures number_samples cost_matrix quality_measure target_value . 59 59 | 1 | 61 | iris | Supervised Classification | active | 10-fold Crossvalidation | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | NaN | NaN | NaN | NaN | . 289 289 | 1 | 61 | iris | Supervised Classification | active | 33% Holdout set | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | NaN | NaN | NaN | NaN | . 1823 1823 | 1 | 61 | iris | Supervised Classification | active | 5 times 2-fold Crossvalidation | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | NaN | NaN | NaN | NaN | . 1939 1939 | 1 | 61 | iris | Supervised Classification | active | 10 times 10-fold Crossvalidation | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | NaN | NaN | NaN | NaN | . 1992 1992 | 1 | 61 | iris | Supervised Classification | active | Leave one out | predictive_accuracy | 61 | class | ... | 5 | 150 | 0 | 0 | 4 | 1 | NaN | NaN | NaN | NaN | . 5 rows × 24 columns . # Collecting all relevant task_ids tasks = df.query(&quot;task_type==&#39;Supervised Classification&#39;&quot;)[&#39;tid&#39;].to_numpy() print(len(tasks)) . 13 . That settles the task component too. Notice how for one dataset (61), we obtain 11 task IDs which are of interest to us. This should illustrate the one-to-many relationship that dataset-task components can have. We have 2 more components to explore - flows, runs. We could list out all possible flows and filter out the ones we want, i.e., Random Forest. However, let us instead fetch all the evaluations made on the Iris dataset using the 11 tasks we collected above. . We shall subsequently work with the scikit-learn based task which has been uploaded/used the most. We shall then further filter out the list of evaluations from the selected task (task_id=59 in this case), depending on if Random Forest was used. . # Listing all evaluations made on the 11 tasks collected above # with evaluation metric as &#39;predictive_accuracy&#39; task_df = openml.evaluations.list_evaluations(function=&#39;predictive_accuracy&#39;, task=tasks, output_format=&#39;dataframe&#39;) task_df.head() . run_id task_id setup_id flow_id flow_name data_id data_name function upload_time uploader uploader_name value values array_data . 0 81 | 59 | 12 | 67 | weka.BayesNet_K2(1) | 61 | iris | predictive_accuracy | 2014-04-07 00:05:11 | 1 | janvanrijn@gmail.com | 0.940000 | None | None | . 1 161 | 59 | 13 | 70 | weka.SMO_PolyKernel(1) | 61 | iris | predictive_accuracy | 2014-04-07 00:55:32 | 1 | janvanrijn@gmail.com | 0.960000 | None | None | . 2 234 | 59 | 1 | 56 | weka.ZeroR(1) | 61 | iris | predictive_accuracy | 2014-04-07 01:33:24 | 1 | janvanrijn@gmail.com | 0.333333 | None | None | . 3 447 | 59 | 6 | 61 | weka.REPTree(1) | 61 | iris | predictive_accuracy | 2014-04-07 06:26:27 | 1 | janvanrijn@gmail.com | 0.926667 | None | None | . 4 473 | 59 | 18 | 77 | weka.LogitBoost_DecisionStump(1) | 61 | iris | predictive_accuracy | 2014-04-07 06:39:27 | 1 | janvanrijn@gmail.com | 0.946667 | None | None | . # Filtering based on sklearn (scikit-learn) task_df = task_df[task_df[&#39;flow_name&#39;].str.contains(&quot;sklearn&quot;)] task_df.head() . run_id task_id setup_id flow_id flow_name data_id data_name function upload_time uploader uploader_name value values array_data . 144 1849043 | 59 | 29015 | 5500 | sklearn.ensemble.forest.RandomForestClassifier... | 61 | iris | predictive_accuracy | 2017-03-03 17:10:12 | 1 | janvanrijn@gmail.com | 0.946667 | None | None | . 145 1853409 | 59 | 30950 | 5873 | sklearn.pipeline.Pipeline(Imputer=openml.utils... | 61 | iris | predictive_accuracy | 2017-03-21 22:08:01 | 1 | janvanrijn@gmail.com | 0.960000 | None | None | . 146 6130126 | 59 | 4163633 | 7108 | sklearn.model_selection._search.RandomizedSear... | 61 | iris | predictive_accuracy | 2017-08-21 11:07:40 | 1 | janvanrijn@gmail.com | 0.960000 | None | None | . 147 6130128 | 59 | 4163634 | 7108 | sklearn.model_selection._search.RandomizedSear... | 61 | iris | predictive_accuracy | 2017-08-21 11:08:06 | 1 | janvanrijn@gmail.com | 0.946667 | None | None | . 148 6715383 | 59 | 4747289 | 7117 | sklearn.model_selection._search.RandomizedSear... | 61 | iris | predictive_accuracy | 2017-09-01 02:56:44 | 1 | janvanrijn@gmail.com | 0.960000 | None | None | . # Counting frequency of the different tasks used to # solve Iris as a supervised classification using scikit-learn task_df[&#39;task_id&#39;].value_counts() . 59 1985 10107 25 289 1 Name: task_id, dtype: int64 . # Retrieving the most used task t = openml.tasks.get_task(59) t . OpenML Classification Task ========================== Task Type Description: https://www.openml.org/tt/1 Task ID..............: 59 Task URL.............: https://www.openml.org/t/59 Estimation Procedure.: crossvalidation Evaluation Measure...: predictive_accuracy Target Feature.......: class # of Classes.........: 3 Cost Matrix..........: Available . # Filtering for only task_id=59 task_df = task_df.query(&quot;task_id==59&quot;) . # Filtering based on Random Forest task_rf = task_df[task_df[&#39;flow_name&#39;].str.contains(&quot;RandomForest&quot;)] task_rf.head() . run_id task_id setup_id flow_id flow_name data_id data_name function upload_time uploader uploader_name value values array_data . 144 1849043 | 59 | 29015 | 5500 | sklearn.ensemble.forest.RandomForestClassifier... | 61 | iris | predictive_accuracy | 2017-03-03 17:10:12 | 1 | janvanrijn@gmail.com | 0.946667 | None | None | . 145 1853409 | 59 | 30950 | 5873 | sklearn.pipeline.Pipeline(Imputer=openml.utils... | 61 | iris | predictive_accuracy | 2017-03-21 22:08:01 | 1 | janvanrijn@gmail.com | 0.960000 | None | None | . 146 6130126 | 59 | 4163633 | 7108 | sklearn.model_selection._search.RandomizedSear... | 61 | iris | predictive_accuracy | 2017-08-21 11:07:40 | 1 | janvanrijn@gmail.com | 0.960000 | None | None | . 147 6130128 | 59 | 4163634 | 7108 | sklearn.model_selection._search.RandomizedSear... | 61 | iris | predictive_accuracy | 2017-08-21 11:08:06 | 1 | janvanrijn@gmail.com | 0.946667 | None | None | . 190 6946499 | 59 | 4978397 | 7109 | sklearn.pipeline.Pipeline(imputation=openmlstu... | 61 | iris | predictive_accuracy | 2017-09-02 22:06:32 | 1 | janvanrijn@gmail.com | 0.920000 | None | None | . Retrieving top-performing models from&#160;OpenML . Since we are an ambitious bunch of ML practitioners who settle for nothing but the best, and also since most results will not be considered worth the effort if not matching or beating state-of-the-art, we shall aim for the best scores. We&#39;ll sort the filtered results we obtained based on the score or &#39;value&#39; and then extract the components from that run - task and flow. . task_rf.sort_values(by=&#39;value&#39;, ascending=False).head() . run_id task_id setup_id flow_id flow_name data_id data_name function upload_time uploader uploader_name value values array_data . 3549 523926 | 59 | 3526 | 2629 | sklearn.ensemble.forest.RandomForestClassifier(8) | 61 | iris | predictive_accuracy | 2016-02-11 22:05:23 | 869 | p.gijsbers@student.tue.nl | 0.966667 | None | None | . 4353 8955370 | 59 | 6890988 | 7257 | sklearn.ensemble.forest.RandomForestClassifier... | 61 | iris | predictive_accuracy | 2018-04-06 16:32:22 | 3964 | clear.tsai@gmail.com | 0.960000 | None | None | . 3587 1852682 | 59 | 29263 | 5500 | sklearn.ensemble.forest.RandomForestClassifier... | 61 | iris | predictive_accuracy | 2017-03-15 22:55:18 | 1022 | rso@randalolson.com | 0.960000 | None | None | . 4375 8886608 | 59 | 6835139 | 7961 | sklearn.pipeline.Pipeline(Imputer=sklearn.prep... | 61 | iris | predictive_accuracy | 2018-03-17 16:46:27 | 5032 | rashmi.kamath01@gmail.com | 0.960000 | None | None | . 3107 1843272 | 59 | 24071 | 4830 | sklearn.ensemble.forest.RandomForestClassifier... | 61 | iris | predictive_accuracy | 2016-12-08 20:10:03 | 2 | joaquin.vanschoren@gmail.com | 0.960000 | None | None | . # Fetching the Random Forest flow with the best score f = openml.flows.get_flow(2629) f . OpenML Flow =========== Flow ID.........: 2629 (version 8) Flow URL........: https://www.openml.org/f/2629 Flow Name.......: sklearn.ensemble.forest.RandomForestClassifier Flow Description: Flow generated by openml_run Upload Date.....: 2016-02-11 21:17:08 Dependencies....: None . # Fetching the run with the best score for # Random Forest on Iris r = openml.runs.get_run(523926) r . OpenML Run ========== Uploader Name...: Pieter Gijsbers Uploader Profile: https://www.openml.org/u/869 Metric..........: predictive_accuracy Result..........: 0.966667 Run ID..........: 523926 Run URL.........: https://www.openml.org/r/523926 Task ID.........: 59 Task Type.......: Supervised Classification Task URL........: https://www.openml.org/t/59 Flow ID.........: 2629 Flow Name.......: sklearn.ensemble.forest.RandomForestClassifier(8) Flow URL........: https://www.openml.org/f/2629 Setup ID........: 3526 Setup String....: None Dataset ID......: 61 Dataset URL.....: https://www.openml.org/d/61 . Okay, let&#39;s take a pause and re-assess. From multiple users across the globe, who had uploaded runs to OpenML, for a Random Forest run on the Iris, the best score seen till now is 96.67%. That is certainly better than the naive model we built at the beginning to achieve 95.33%. We had used a basic 10-fold cross-validation to evaluate a Random Forest of 10 trees with a max depth of 2. Let us see, what the best run uses and if it differs from our approach. . # The scoring metric used t.evaluation_measure . &#39;predictive_accuracy&#39; . # The methodology used for estimations t.estimation_procedure . {&#39;type&#39;: &#39;crossvalidation&#39;, &#39;parameters&#39;: {&#39;number_repeats&#39;: &#39;1&#39;, &#39;number_folds&#39;: &#39;10&#39;, &#39;percentage&#39;: &#39;&#39;, &#39;stratified_sampling&#39;: &#39;true&#39;}, &#39;data_splits_url&#39;: &#39;https://www.openml.org/api_splits/get/59/Task_59_splits.arff&#39;} . # The model used f.name . &#39;sklearn.ensemble.forest.RandomForestClassifier&#39; . # The model parameters for param in r.parameter_settings: name, value = param[&#39;oml:name&#39;], param[&#39;oml:value&#39;] print(&quot;{:&lt;25} : {:&lt;10}&quot;.format(name, value)) . warm_start : False oob_score : False n_jobs : 1 verbose : 0 max_leaf_nodes : None bootstrap : True min_samples_leaf : 1 n_estimators : 10 min_samples_split : 2 min_weight_fraction_leaf : 0.0 criterion : gini random_state : None max_features : auto max_depth : None class_weight : None . As evident, our initial approach is different on two fronts. We didn&#39;t explicitly use stratified sampling for our cross-validation. While the Random Forest hyperparameters are slightly different too (max_depth=None). That definitely sounds like a to-do, however, there is no reason why we should restrict ourselves to Random Forests. Remember, we are aiming big here. Given the number of OpenML users, there must be somebody who got a better score on Iris with some other model. Let us then retrieve that information. Programmatically, of course. . In summary, we are now going to sort the performance of all scikit-learn based models on Iris dataset as per the task definition with task_id=59. . # Fetching top performances task_df.sort_values(by=&#39;value&#39;, ascending=False).head() . run_id task_id setup_id flow_id flow_name data_id data_name function upload_time uploader uploader_name value values array_data . 3630 2039748 | 59 | 180922 | 6048 | sklearn.pipeline.Pipeline(dualimputer=helper.d... | 61 | iris | predictive_accuracy | 2017-04-09 01:09:01 | 1104 | jmapvhoof@gmail.com | 0.986667 | None | None | . 3631 2039750 | 59 | 180924 | 6048 | sklearn.pipeline.Pipeline(dualimputer=helper.d... | 61 | iris | predictive_accuracy | 2017-04-09 01:17:39 | 1104 | jmapvhoof@gmail.com | 0.986667 | None | None | . 3624 2012939 | 59 | 157622 | 6048 | sklearn.pipeline.Pipeline(dualimputer=helper.d... | 61 | iris | predictive_accuracy | 2017-04-06 23:29:28 | 1104 | jmapvhoof@gmail.com | 0.986667 | None | None | . 3618 2012930 | 59 | 157613 | 6048 | sklearn.pipeline.Pipeline(dualimputer=helper.d... | 61 | iris | predictive_accuracy | 2017-04-06 23:00:24 | 1104 | jmapvhoof@gmail.com | 0.986667 | None | None | . 3626 2012941 | 59 | 157624 | 6048 | sklearn.pipeline.Pipeline(dualimputer=helper.d... | 61 | iris | predictive_accuracy | 2017-04-07 01:36:00 | 1104 | jmapvhoof@gmail.com | 0.986667 | None | None | . # Fetching best performing flow f = openml.flows.get_flow(6048) f . OpenML Flow =========== Flow ID.........: 6048 (version 1) Flow URL........: https://www.openml.org/f/6048 Flow Name.......: sklearn.pipeline.Pipeline(dualimputer=helper.dual_imputer.DualImputer,nusvc=sklearn.svm.classes.NuSVC) Flow Description: Automatically created scikit-learn flow. Upload Date.....: 2017-04-06 22:42:59 Dependencies....: sklearn==0.18.1 numpy&gt;=1.6.1 scipy&gt;=0.9 . # Fetching best performing run r = openml.runs.get_run(2012943) # The model parameters for param in r.parameter_settings: name, value = param[&#39;oml:name&#39;], param[&#39;oml:value&#39;] print(&quot;{:&lt;25} : {:&lt;10}&quot;.format(name, value)) . steps : [(&#39;DualImputer&#39;, &lt;helper.dual_imputer.DualImputer object at 0x7ff618e4d908&gt;), (&#39;nusvc&#39;, NuSVC(cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, nu=0.3, probability=True, random_state=3, shrinking=True, tol=3.2419092644286417e-05, verbose=False))] cache_size : 200 class_weight : None coef0 : 0.0 decision_function_shape : None degree : 3 gamma : auto kernel : linear max_iter : -1 nu : 0.3 probability : True random_state : 3 shrinking : True tol : 3.24190926443e-05 verbose : False . The highest score obtained among the uploaded results is 98.67% using a variant of SVM. However, if we check the corresponding flow description, we see that it is using an old scikit-learn version (0.18.1) and therefore may not be possible to replicate the exact results. However, in order to improve from our score of 95.33%, we should try running a nu-SVC on the same problem and see where we stand. Let&#39;s go for it. Via OpenML, of course. . Running best performing flow on the required&#160;task . import openml import numpy as np from sklearn.svm import NuSVC . # Building the NuSVC model object with parameters found clf = NuSVC(cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, nu=0.3, probability=True, random_state=3, shrinking=True, tol=3.2419092644286417e-05, verbose=False) . # Obtaining task used earlier t = openml.tasks.get_task(59) t . OpenML Classification Task ========================== Task Type Description: https://www.openml.org/tt/1 Task ID..............: 59 Task URL.............: https://www.openml.org/t/59 Estimation Procedure.: crossvalidation Evaluation Measure...: predictive_accuracy Target Feature.......: class # of Classes.........: 3 Cost Matrix..........: Available . # Running the model on the task # Internally, the model will be made into # an OpenML flow and we can choose to retrieve it r, f = openml.runs.run_model_on_task(model=clf, task=t, upload_flow=False, return_flow=True) f . OpenML Flow =========== Flow Name.......: sklearn.svm.classes.NuSVC Flow Description: Nu-Support Vector Classification. Similar to SVC but uses a parameter to control the number of support vectors. The implementation is based on libsvm. Dependencies....: sklearn==0.21.3 numpy&gt;=1.6.1 scipy&gt;=0.9 . # To obtain the score (without uploading) ## r.publish() can be used to upload these results ## need to sign-in to https://www.openml.org/ score = [] evaluations = r.fold_evaluations[&#39;predictive_accuracy&#39;][0] for key in evaluations: score.append(evaluations[key]) print(np.mean(score)) . 0.9866666666666667 . Lo and behold! We hit the magic number. I personally would have never tried out NuSVC and would have stuck around tweaking hyperparameters of the Random Forest. This is a new discovery of sorts for sure. I wonder though if anybody has tried XGBoost on Iris? . In any case, we can now upload the results of this run to OpenML using: . r.publish() . OpenML Run ========== Uploader Name: None Metric.......: None Run ID.......: 10464835 Run URL......: https://www.openml.org/r/10464835 Task ID......: 59 Task Type....: None Task URL.....: https://www.openml.org/t/59 Flow ID......: 18579 Flow Name....: sklearn.svm.classes.NuSVC Flow URL.....: https://www.openml.org/f/18579 Setup ID.....: None Setup String.: Python_3.6.9. Sklearn_0.21.3. NumPy_1.16.4. SciPy_1.4.1. NuSVC(cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;, max_iter=-1, nu=0.3, probability=True, random_state=3, shrinking=True, tol=3.241909264428642e-05, verbose=False) Dataset ID...: 61 Dataset URL..: https://www.openml.org/d/61 . One would need to sign-in to https://www.openml.org/ and generate their respective apikey. The results would then be available for everyone to view and who knows, you can have your name against the best-ever performance measured on the Iris dataset! . . This post was in no ways intended to be a be-all-end-all guide to OpenML. The primary goal was to help form an acquaintance with the OpenML terminologies, introduce the API, establish connections with the general ML practices, and give a sneak-peek into the potential benefits of working together as a community. For a better understanding of OpenML, please explore the documentation. If one desires to continue from the examples given in this post and explore further, kindly refer to the API. . OpenML-Python is an open-source project and contributions from everyone in the form of Issues and Pull Requests are most welcome. Contribution to the OpenML community is in fact not limited to code contribution. Every single user can make the community richer by sharing data, experiments, results, using OpenML. . As ML practitioners, we may be dependent on tools for our tasks. However, as a collective, we can juice out its potential to a larger extent. Let us together, make ML more transparent, more democratic! . . Special thanks to Heidi, Bilge, Sahithya, Matthias, Ashwin for the ideas, feedback, and support. . . Related readings: . To get started with OpenML-Python | OpenML-Python Github | The OpenML website | Miscellaneous reading on OpenML | To get in touch! | .",
            "url": "https://openml.github.io/blog/2020/06/08/OpenML-Machine-Learning-as-a-community.html",
            "relUrl": "/2020/06/08/OpenML-Machine-Learning-as-a-community.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Reproducible deep learning with OpenML",
            "content": "Deep learning is facing a reproducibility crisis right now[1]. The scale of experiments and there are numerous hyperparameters that affect performance, which makes it hard for the author to write a reproducibility document. The current best way to make an experiment reproducible is to upload the code. However, that&#39;s not optimal in a lot of situations where we have a huge undocumented codebase and someone would like to just reproduce the model. OpenML[2] is an online machine learning platform for sharing and organizing data, machine learning algorithms and experiments. Until now we only provided support for classical machine learning and libraries like Sklearn and MLR. We see there is a huge need for reproducible deep learning now. To solve this issue OpenML is launching its deep learning plugins for popular deep learning libraries like Keras, MXNet, and Pytorch. . Here we have a small tutorial on how to use our pytorch extension with MNIST dataset. . Setup To install openml and openml pytorch extension execute this instruction in your terminal pip install openml openml_pytorch . !pip install openml openml_pytorch . Collecting openml Downloading https://files.pythonhosted.org/packages/68/5b/cd32bb85651eccebfb489cc6ef7f060ce0f62350a6239127e398313090cc/openml-0.10.2.tar.gz (158kB) |████████████████████████████████| 163kB 9.3MB/s Collecting openml_pytorch Downloading https://files.pythonhosted.org/packages/5b/a4/8c69a041e7929d93460db17cf276abfb7b49af9c3d5077bee1c52101ba4c/openml_pytorch-0.0.1-py3-none-any.whl Collecting liac-arff&gt;=2.4.0 Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz Collecting xmltodict Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from openml) (2.21.0) Requirement already satisfied: scikit-learn&gt;=0.18 in /usr/local/lib/python3.6/dist-packages (from openml) (0.22.2.post1) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from openml) (2.8.1) Requirement already satisfied: pandas&gt;=0.19.2 in /usr/local/lib/python3.6/dist-packages (from openml) (1.0.3) Requirement already satisfied: scipy&gt;=0.13.3 in /usr/local/lib/python3.6/dist-packages (from openml) (1.4.1) Requirement already satisfied: numpy&gt;=1.6.2 in /usr/local/lib/python3.6/dist-packages (from openml) (1.18.2) Collecting torch==1.2.0 Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB) |████████████████████████████████| 748.9MB 14kB/s Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;openml) (3.0.4) Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;openml) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;openml) (2020.4.5.1) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;openml) (2.8) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn&gt;=0.18-&gt;openml) (0.14.1) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil-&gt;openml) (1.12.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.19.2-&gt;openml) (2018.9) Building wheels for collected packages: openml, liac-arff Building wheel for openml (setup.py) ... done Created wheel for openml: filename=openml-0.10.2-cp36-none-any.whl size=190318 sha256=50c2fd823d13904f246bf30997a2464379c393377cfd77f74b5dace4935db99c Stored in directory: /root/.cache/pip/wheels/71/ec/5f/aaad9e184680b0b8f1a02ff0ec640cace5adf5bff7bb0af1b4 Building wheel for liac-arff (setup.py) ... done Created wheel for liac-arff: filename=liac_arff-2.4.0-cp36-none-any.whl size=13335 sha256=fca5bc5e07e3fe4f591cbe79968a42e43f535b3bd3e4be77b62c901d46feaaa9 Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203 Successfully built openml liac-arff ERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you&#39;ll have torch 1.2.0 which is incompatible. Installing collected packages: liac-arff, xmltodict, openml, torch, openml-pytorch Found existing installation: torch 1.4.0 Uninstalling torch-1.4.0: Successfully uninstalled torch-1.4.0 Successfully installed liac-arff-2.4.0 openml-0.10.2 openml-pytorch-0.0.1 torch-1.2.0 xmltodict-0.12.0 . Let&#39;s import the necessary libraries . import torch.nn import torch.optim import openml import openml_pytorch import logging . Set the apikey for openml python library, you can find your api key in your openml.org account . openml.config.apikey = &#39;key&#39; . Define a sequential network that does initial image reshaping and normalization model . processing_net = torch.nn.Sequential( openml_pytorch.layers.Functional(function=torch.Tensor.reshape, shape=(-1, 1, 28, 28)), torch.nn.BatchNorm2d(num_features=1) ) print(processing_net) . Sequential( (0): Functional() (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . Define a sequential network that does the extracts the features from the image. . features_net = torch.nn.Sequential( torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5), torch.nn.LeakyReLU(), torch.nn.MaxPool2d(kernel_size=2), torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5), torch.nn.LeakyReLU(), torch.nn.MaxPool2d(kernel_size=2), ) print(features_net) . Sequential( (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1)) (1): LeakyReLU(negative_slope=0.01) (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1)) (4): LeakyReLU(negative_slope=0.01) (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) . Define a sequential network that flattens the features and compiles the results into probabilities for each digit. . results_net = torch.nn.Sequential( openml_pytorch.layers.Functional(function=torch.Tensor.reshape, shape=(-1, 4 * 4 * 64)), torch.nn.Linear(in_features=4 * 4 * 64, out_features=256), torch.nn.LeakyReLU(), torch.nn.Dropout(), torch.nn.Linear(in_features=256, out_features=10), ) print(results_net) . Sequential( (0): Functional() (1): Linear(in_features=1024, out_features=256, bias=True) (2): LeakyReLU(negative_slope=0.01) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=256, out_features=10, bias=True) ) . The main network, composed of the above specified networks. . model = torch.nn.Sequential( processing_net, features_net, results_net ) print(model) . Sequential( (0): Sequential( (0): Functional() (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1)) (1): LeakyReLU(negative_slope=0.01) (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1)) (4): LeakyReLU(negative_slope=0.01) (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (2): Sequential( (0): Functional() (1): Linear(in_features=1024, out_features=256, bias=True) (2): LeakyReLU(negative_slope=0.01) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=256, out_features=10, bias=True) ) ) . Download the OpenML task for the mnist 784 dataset. . task = openml.tasks.get_task(3573) . Run the model on the task and publish the results on openml.org . run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) run.publish() print(&#39;URL for run: %s/run/%d&#39; % (openml.config.server, run.run_id)) . URL for run: https://www.openml.org/api/v1/xml/run/10452577 . By going to the published URL you can check the model performance and other metadata . . We hope that openml deep learning plugins can help in reproducing deep learning experiments and provide a universal reproducibility platform for the experiments. Here are the links of all supported deep learning plugins right now: . MXNet: https://github.com/openml/openml-mxnet . | Keras: https://github.com/openml/openml-keras . | Pytorch: https://github.com/openml/openml-pytorch . | ONNX: https://github.com/openml/openml-onnx . | . There are examples of how to use these libraries in the Github repos. These libraries are in the development stage right now so we would appreciate any feedback on Github issues of these libraries. Links: . https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/ | https://www.openml.org |",
            "url": "https://openml.github.io/blog/openml/deep%20learning/2020/05/06/Reproducible-deep-learning-with-OpenML.html",
            "relUrl": "/openml/deep%20learning/2020/05/06/Reproducible-deep-learning-with-OpenML.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Finding a standard dataset format for machine learning",
            "content": "With OpenML, we aim to take a worry-free, &#39;zen&#39;-like approach to working with machine learning datasets, making them easy and reliable to use. We want to offer training data that can be easily (or automatically) used. As such, you can now load any dataset and start building models without any manual intervention. For historical reasons, we have done this by internally storing all data in the ARFF data format, which is a simple single-table format, including meta-data such as the correct feature data types. . However, this format is currently holding us back: it is not ideal for storing large datasets, the format is only loosely defined causing different parsers to behave differentlyeDd, and many new machine learning tasks require multi-table data. For instance, image segmentation or object detection tasks have both images and varying amounts of annotations per image. . Hence, we started searching for a better data format that we can use to store (and share) machine learning datasets in the foreseeable future. This blog post presents out process and insights. We would love to hear your thoughts and experiences before we make any decision on how to move forward. . Scope . We first defined the general scope of the usage of the format: . We mainly need a format that is useful for data storage and transmission. We can always convert data during upload or download in OpenML&#39;s client APIs. For instance, people may upload a Python pandas dataframe to OpenML, and later get the same dataframe back, without realizing or caring how the data was stored in the meantime. If people want to store the data locally, they can download it in the format they like (e.g. a memory-mapped format like Arrow/Feather for fast reading or TFRecords for people using TensorFlow). Additional code can facilitate such conversions. | The format should allow storing most machine learning datasets, including images, video, audio, text, graphs, and multi-tabular data such as object recognition tasks and relational data. | To make the data easy to use, there should be a standard way to represent specific types of data (i.e. a fixed schema that can be verified), so that we can easily read and store datasets in a uniform way. For instance, all tabular data should be stored in a uniform way. | . Impact on OpenML (simplicity, maintenance) . Since OpenML is a community project, we want to keep it as easy as possible to use and maintain: . We prefer a single internal data format rather than multiple ones, since the latter would impose more maintenance on both server-side and client-side. | Even if we choose a flexible data format, this does not mean that we will store arbitrary dataset files for which we cannot guarantee data quality and easy of use. | We will require machine-readable schemas(in a specific language) that describe how a certain &#39;type&#39; data is formatted, to ensure ease of use and maintainability. Examples would be a schema for tabular data, a schema for annotated image data, etc. Every dataset should specify the schema it satisfied. We should be able to validate whether the dataset indeed satisfies the schema. Every OpenML task should be mapped to a schema, so that it is clear how the data must be used in training and testing. OpenML&#39;s clients can then use these schema to handle new types of data. | OpenML would have a set of allowed schemas. We will stick to tabular data until other schemas are defined, and we are sure that we can validate and handle those datasets. | When no agreed upon schema exists, we could offer a forum for the community to discuss and agree on a standard schema, in collaboration with other initiatives (e.g. frictionlessdata). [For instance, new schemas could be created in a github repo to allow people to do create pull requests. They could be effectively used once they are merged.] | . Requirements . To draw up a shortlist of data formats, we used the following requirements: . Maintenance. We are still a small community so we would prefer a format which is stable and fully maintained over something which we would have to maintain ourselves. | Support for the format in various programming languages, including well-maintained and stable libraries. | Support for reading the data without copying everything into memory (e.g. incremental reads/writes), and for subselecting parts of the data and operating only on that. | Ideally, there is a way to detect bitflip errors during storage or transmission. | The dataset format should support multiple &quot;resources&quot; to support cases when we would like to store collections of files or multiple relational tables. | Support for storing binary blobs and vectors of different lengths. | Anything else being equal, the dataset format should be compact. | Ideally, there should be support for storing sparse data. | A nice to have is that we can store some meta-data inside the file. OpenML can generate more extensive meta-data on the fly, but storing a minimal set inside the file may be useful. | . Shortlist . We decided to investigate the following formats in more detail: . Apache Arrow / Feather . Benefits: . Great for locally caching files after download | Memory-mapped, so very fast reads | . Drawbacks: . Not stable enough yet and not ideal for long-term storage. The authors also discourage it for long-term storage. | Limited to one data structure per file, but that data structure can be complex (e.g. dict). | . Parquet . Benefits: . Used in industry a lot, especially in combination with Apache Spark. Good community of practice | Well-supported and maintained, but not all Parquet features are supported in every library. E.g. the python library does not support partial read/writes. | Simple structure | Built-in compression (columnar storage), very efficient long-term data storage | . Drawbacks: . The different parsers (e.g. Parquet support inside Arrow, fastparquet) implement different parts of the Parquet format (and different set of compression algorithms), meaning that the output may not be compatiblewith other parsers (in other languages). | Support limited to single-table storage. There is good support to convert to and from pandas DataFrames, but there is less support for more complex data structures. Also, Parquet files created by other libraries might not be readable into pandas. More complicated data schemas might be possible, but are not supported in Python. For instance, there doesn&#39;t seem to be an apparent way to store an object detection dataset (with images and annotations) as a single parquet file. | Limited support for incremental reading/writing. None of the parsers we looked at (e.g. Parquet support inside Arrow, fastparquet) allows incremental writes, which may be an issue for large datasets when we (or end users) cannot load the data into memory. We can easily store large datasets in multiple (partitioned) parquet files, but that would make them less easy to use in OpenML. | . SQLite . Benefits: . SQLite was the easiest to use. It was comparably fast to HDF5 in our tests. | Very good support in all languages. For instance, it is built-in in Python. | Very flexible access to parts of the data. SQL queries can be used to select any subset of the data. | . Drawback: . It supports only 2000 columns, and we have quite a few datasets with more than 2000 features. Hence, storing large tabular data will require mapping data differently, which would add a lot of additional complexity. | Writing SQL queries requires knowledge of the internal data structure (tables, keys,…). | . HDF5 . Benefits: . Very good support in all languages. Has well-tested parsers, all using the same C implementation. | Widely accepted format in the deep learning community to store data and models. | Widely accepted format in many scientific domains (e.g. astronomy, bioinformatics,…) | Provides built-in compression. Constructing and loading datasets was reasonably fast. | Very flexible. Should allow to store any machine learning dataset as a single file. | Allows easy inclusion of meta-data inside the file, creating a self-contained file. | Self-descriptive: the structure of the data can be easily read programmatically. For instance, &#39;h5dump -H -A 0 mydata.hdf5&#39; will give you a lot of detail on the structure of the dataset. | . Drawbacks: . Complexity. We cannot make any a priori assumptions about how the data is structured. We need to define schema and implement code that automatically validates that a dataset follows a specific schema (e.g. e.g. using h5dump to see whether it holds a single dataframe that we could load into pandas). | The format has a very long and detailed specification. While parsers exist we don&#39;t really know whether they are fully compatible with each other. There may be unknown bugs. | . CSV . Benefits: . Very good support in all languages. | Easy to use, requires very little additional tooling | Easy versioning with git LFS. Changes in different versions can be observed with a simple git diff. | The current standard used in frictionlessdata. | There exist schema to express certain types of data in CSV (see frictionlessdata). | . Drawbacks: . Not very efficient for storing floating point numbers | Not ideal for very, very large datasets (when data does not fit in memory/disk) | Many different dialects exist. We need to decide on a standardized dialect and enforce that only that dialect is used on OpenML (https://frictionlessdata.io/specs/csv-dialect/). The dialect specified in RFC4180, which uses the comma as delimiter and the quotation mark as quote character, is often recommended. | . Overview .   Parquet HDF5 SQLite CSV . Consistency across different platforms | Check or clarify | ✅ | ✅ | ✅(dialect) | . Support and documentation | ✅ | ✅ | ✅ | ✅ | . Supports very large and high-dimensional datasets | ✅ | ✅ | ❌(limited nr. columns per table) | ✅ Storing tensors requires flattening. | . Simplicity | ✅ | ❌(basically full file system) | ✅(database) | ✅ | . Metadata support | Only minimal | ✅ | ✅ | ❌(requires separate metadata file) | . Maintenance | Apache project, open and quite active | Closed group, but active community on Jira and conferences | Run by a company. Community interaction via email list. | ✅ | . Available examples of usage in ML | ✅ | ✅ | ❌ | ✅ | . Allows incremental reads/writes | Yes, but not supported by current Python libs | ✅ | ✅ | Yes (but not random access) | . Flexibility | Only tabular data supported | Very flexible, maybe too flexible | Relational multi-table | Only tabular | . Performance benchmarks . There exist some prior benchmarks (here and here) on storing dataframes. These only consider single-table datasets. We also ran our own benchmark to compare the writing performance of those data formats for more complex machine learning datasets, such as pedestrian detection and music analysis (note that we could not find a way to store these in one file in Parquet). Reading benchmarks have yet to be done. . Version control . Version control for large datasets is tricky. For CSV, we could use git LFS store the datasets and have automated versioning of datasets. The binary formats do not allow us to track changes in the data, only to recover the exact versions of the datasets you want (and their metadata). . We found it quite easy to export all OpenML dataset to GitLab: https://gitlab.com/data/d/openml . We need your help! If we have missed any format we should investigate, or misunderstood those we have investigated, or missed some best practice, please tell us. . Email: openmlhq@googlegroups.com . Contributors to this blog post: . Mitar Milutinovic, Prabhant Singh, Joaquin Vanschoren, Pieter Gijsbers .",
            "url": "https://openml.github.io/blog/openml/data/2020/03/23/Finding-a-standard-dataset-format-for-machine-learning.html",
            "relUrl": "/openml/data/2020/03/23/Finding-a-standard-dataset-format-for-machine-learning.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "OpenML workshop at Dagstuhl",
            "content": "October 2019 . Twice a year the OpenML community meets for a workshop/hackathon/unconference. We improve the platform, discuss and learn for 5 days. If that sounds interesting to you, get in touch or follow updates on meet.openml.org. . This time the workshop took place at Dagstuhl, a great place for Seminars. . . . We had several breakouts where workshop attendees can join to learn, discuss and progress OpenML. In the following we discuss some of the topics we touched. . Science projects . Brainstorm on scientific projects to do with OpenML. Prioritize impactful, well-defined research ideas. We came up with quite a long list of very promising research questions that should actually be quite easy to answer based on OpenML. Many of these are along the lines of empirically providing evidence to verify or bust commonly-held beliefs in the community. Frank would gladly hire a strong postdoc (or quite independent PhD student) and maybe a research engineer, to work on these scientific questions under the umbrella of “evidence-based machine learning with OpenML”. . Benchmarking using OpenML . Define guidelines on how to define world-class benchmarks and how to run them. . Diverse datasets . New and more diverse datasets. . Dataset quality . How to measure data quality and how to improve the quality of datasets on OpenML. . OpenML use cases for novices . Shortlist common use cases and start writing accessible blog posts for novice users. If you are a OpenML newbie we need your help with this topic. . The output of this breakout will be at least one blog post. Keep an eye out for them here :) . Running a competition using OpenML . We assessed the current issues with running in-class competitions for teaching purposes using OpenML (biggest one: easy use for non-developers) and brainstormed on a new competition format with related competitions, one for each component of a solution, such as HPO, creating good meta-features, creating a good search space, etc. . Planning future workshops . Decide on location and timing for the next couple of workshops. The next OpenML workshop will be in Spring (week of March 30th or week of April 14th) close to Munich. For updates check http://meet.openml.org. The workshop will be cohosted with some other Open Source Machine Learning projects. . Furthermore a workshop in Austin is being planned for next summer and a datathon is in planning. We are planning to organize dev sprints at various PyCons next year. . New frontend . Feedback session on new frontend, Additional visualization for datasets . Future of client APIs . Currently, a lot of resources are bound developing different client APIs, such as the Python API, R API and Java API. We discussed how we can better share work and code between the different APIs and the server. For now we are working on automatically generating the Swagger API documentation from the PHP function documentation, which in turn will allow us to generate (documented) parts of the APIs, reduce the need for maintenance and will help to spread updates on the API faster. . Flow 2.0 design . Current flow design used in OpenML was inspired by Weka, but through time many limitations have been identified, primarily that existing flow does not allow duplicate use of same component and that it cannot express DAG-based ML programs. We started working on a new specification building on insights from mlr3 and d3m projects, centered around DAG representation. Current plan is to prepare a draft specification and implement prototype converters between other systems and this new specification. Once we do that we will re-evaluate the amount of work it took to build those prototypes and how well the specification satisfied those other systems. . Random Bot . The LRZ in Munich provided us with CPU time during the SuperMUC-NG supercomputer test phase, which we used to perform experiments of popular machine learning algorithms with random hyperparameter configurations. This resulted in millions of data points on more than a hundred datasets that we will analyse and publish. The data can be used to learn about typical behaviour of different learners across different datasets, and to construct surrogate models for tuning algorithm benchmarks. . AutoML Benchmark (Janek) . A study was created containing 76 binary and multiclass tasks of reasonable difficulty. . These can be used as a more difficult version of OpenML-100 or in amlb a platform for reproducible benchmarking of AutoML systems. . R API . Short session on how the R api will (need to) change. The main issue discussed was that the OpenML R package runs with mlr and breaks when the new package (mlr3) is loaded. We will update the current OpenML R package to work with mlr3. At the same time we are thinking about a vision for a rewrite of the OpenML R package. . Python API . We made a lot of improvements to the Python API over the week, with over 20 PRs merged! We’ve added more examples on how to use the package, fixed bugs, improved documentation and refactored code. In the coming days we’re going to make all these improvements available in a new PyPI release. For those looking for a higher level overview of the package, we will publish a paper next week which highlights use-cases, its software design, and project structure. . Benchmarking paper . We are working on a comprehensive paper using sklearn, mlr and WEKA, which should demonstrate how OpenML can be used for proper benchmarking and analysis. . Guidelines / Overfitting / Comparable Metalearning . There are plans for writing a guidelines and pitfalls paper on benchmarking, meta-overfitting and statistical analysis of results on OpenML. . Data Formats . Currently OpenML supports only tabular data in ARFF data format. This is very limiting for many ML tasks. We discussed and explored other data formats we could use as the future next data format. We will post a separate blog post about our process and insights. . Funding . OpenML is looking for funding (developers). New ideas on obtaining funding are very welcome. We discussed some ideas: American funding (we need a collaboration partner); ALICE / CLAIRE; Companies. We are a foundation now, which might make it easier. . Some of the PIs (in particular Bernd Bischl, Frank Hutter and Dawn Song) in the project offer positions with a mix of ML research and development. Contact them if you are interested! . . . We had some talks at the workshop as well: . Mitar Milutinovic: A short introduction to Data Driven Discovery (D3M) . | Yiwen Zhu and Markus Weimer: Large-scale analysis of Jupyter notebooks . | Martin Binder, Michel Lang, Florian Pfisterer, Bernd Bischl: Pipelining with mlr3 . | . … and lots of fun… . . Wanna join the OpenML community? Get in touch! .",
            "url": "https://openml.github.io/blog/openml/2019/10/24/OpenML-workshop-at-Dagstuhl.html",
            "relUrl": "/openml/2019/10/24/OpenML-workshop-at-Dagstuhl.html",
            "date": " • Oct 24, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Basic components of OpenML",
            "content": "During my PhD, we developed OpenML, an online experiment database for Machine Learning. Researchers are encouraged to upload their experimental results on it, so that these can be reused by anyone. Various high level papers have been published that overview the design goals, benefits and opportunities (for example, at ECML/PKDD 2013, SIGKDD Explorations and JLMR). However, there is no clear overview of the basic components upon which the platform is build. In this blog post I will review these, and discuss some best practises. . Data . One of the core components of OpenML are datasets. People can upload their datasets, and the system automatically organises these on line. An example of a dataset is the well-known Iris dataset. It shows all features, once of these is identified as the ‘default target attribute’, although this concept is flexible. It also shows some automatically computed data qualities (or, meta-features). Each dataset has it’s own unique ID. Information about the dataset, the data features and the data qualities can be obtained automatically by means of the following API functions: . Get all available datasets . | Get dataset (required the data id) . | Get data features (requires the data id) . | Get data qualities (requires the data id) . | . Task types and tasks . A dataset alone does not constitute a scientific task. We must first agree on what types of results are expected to be shared. This is expressed in task types: they define what types of inputs are given, which types of output are expected to be returned, and what protocols should be used. For instance, classification tasks should include well-defined cross-validation procedures, labelled input data, and require predictions as outputs. The collection of all this information together is called a task. The Iris dataset has various tasks defined on it, for example this one. Although the web-interface does not show it, this task formally describes the target attribute that should be modelled (in this case the same as the default target attribute of the dataset, but this is flexible), the quality estimation procedure (10-fold cross-validation), the evaluation measure (predictive accuracy) and the cross-validation folds. Useful API operations include: . Get all available tasks . | Get all available tasks of a given type (e.g. get all Classification tasks, requires the id of the task type) . | Get the details of a task (requires task id) . | . Currently, there are a wide range of task types defined on OpenML, including classification, regression, on line learning, clustering and subgroup discovery. Although this set can be extended, this is currently not a supported API operation (meaning that we will add them by hand). If you interested in task types that are currently not supported, please contact us. . Flows . Tasks can be ‘solved’ by classifiers (or algorithms, workflows, flows). OpenML stores references to these flows. It is important to stress that flows are actually ran on the computer of the user, only meta-information about the flow is stored on OpenML. This information includes basic trivialities such as the creator, toolbox and compilation instructions, but also more formal description about hyper parameter. A flow can also contain subflows, for example, the flow Bagging can have a subflow ‘Decision Tree’ which would make the flow ‘Bagging of Decision Trees’. A flow is distinguished by it’s name and ‘external version’, which are both provided by the uploader. When uploading a flow, it is important to think about a good naming convention for the both, for example, the git commit number could be used as external version, as this uniquely identifies a state of the code. Ideally, when two persons are using the same flow, they will use the same name and external version, so that results of the flows can be compared across tasks. (This is ensured when using the toolboxed in which OpenML is integrated, such as Weka, Scikit Learn and MLR). Useful API functions are: . List all flows . | List all my flows . | Give details about a given flow (requires flow id) . | . Runs . Whenever a flow executes a task, this is called a run. The existence of runs is actually the main contribution of OpenML. Some experiments take weeks to complete, and having the results stored on OpenML helps other researchers resuse the experiments. The task description specifies which information should be uploaded in order to have a valid run, in most cases, for each cross-validation fold the predictions on the test set. This allows OpenML to calculate basic evaluation measures, such as predictive accuracy, ROC curves and many more. Also information about the flow and hyper parameter settings should be provided. Some useful API functions: . List all runs performed on a given task (requires task id, e.g., the iris task is 59) . | Compare two flows on all tasks (requires a comma separated list of flow ids, e.g., 1720, 1721 for comparing k-nn with a decision tree) . | And many more … . | . Usually, the result is in some XML or JSON format (depending on the preference of the user), linking together various task ids, flow ids, etc. In order for this to become meaningful, the user needs to perform other API tasks to get information about what flows were executed, what tasks and datasets were used, etc. Details about this will be provided in another post. . Setups . Every run that is executed by a flow, contains information about the hyper parameter settings of the flow. A setup is the combination of all parameter settings of a given flow. OpenML internally links the result of a given run to a setup id. This way, experiments can be done across hyper parameter settings. For example, . Compare two setups on all tasks (requires a comma separated list of setup ids, e.g., 8994, 8995, 8996 for comparing multiple MLP configurations) | . As setups constitute a complex concept, most of the operations concerning setups are hidden from the user. Hence, not all setup functions are properly documented yet. A later blogpost will detail on these. .",
            "url": "https://openml.github.io/blog/openml/2017/03/03/Basic-components-of-OpenML.html",
            "relUrl": "/openml/2017/03/03/Basic-components-of-OpenML.html",
            "date": " • Mar 3, 2017"
        }
        
    
  
    
        ,"post5": {
            "title": "mlr loves OpenML",
            "content": "OpenML stands for Open Machine Learning and is an online platform, which aims at supporting collaborative machine learning online. It is an Open Science project that allows its users to share data, code and machine learning experiments. . At the time of writing this blog post I am in Eindhoven at an OpenML workshop, where developers and scientists meet to work on improving the project. Some of these people are R users and they (we) are developing an R package that communicates with the OpenML platform. . . OpenML in R . The OpenML R package can list and download data sets and machine learning tasks (prediction challenges). In R one can run algorithms on the these data sets/tasks and then upload the results to OpenML. After successful uploading, the website shows how well the algorithm performs. To run the algorithm on a given task the OpenML R package builds on the mlr package. mlr understands what a task is and can run learners on that task. So all the OpenML package needs to do is convert the OpenML objects to objects mlr understands and then mlr deals with the learning. . A small case study . We want to create a little study on the OpenML website, in which we compare different types of Support Vector Machines. The study gets an ID assigned to it, which in our case is 27. We use the function ksvm (with different settings of the function argument type) from package kernlab, which is integrated in mlr (“classif.ksvm”). . . For details on installing and setting up the OpenML R package please see the guide on GitHub. . Let’s start conducting the study: . Load the packages and list all tasks which have between 100 and 500 observations. | . library(&quot;OpenML&quot;) library(&quot;mlr&quot;) library(&quot;farff&quot;) library(&quot;BBmisc&quot;) dsize = c(100, 500) taskinfo_all = listOMLTasks(number.of.instances = dsize) . Select all supervised classification tasks that do 10-fold cross-validation and choose only one task per data set. To keep the study simple and fast to compute, select only the first three tasks. | . taskinfo_10cv = subset(taskinfo_all, task.type == &quot;Supervised Classification&quot; &amp; estimation.procedure == &quot;10-fold Crossvalidation&quot; &amp; evaluation.measures == &quot;predictive_accuracy&quot; &amp; number.of.missing.values == 0 &amp; number.of.classes %in% c(2, 4)) taskinfo = taskinfo_10cv[1:3, ] . Create the learners we want to compare. | . lrn.list = list( makeLearner(&quot;classif.ksvm&quot;, type = &quot;C-svc&quot;), makeLearner(&quot;classif.ksvm&quot;, type = &quot;kbb-svc&quot;), makeLearner(&quot;classif.ksvm&quot;, type = &quot;spoc-svc&quot;) ) . Run the learners on the three tasks. | . grid = expand.grid(task.id = taskinfo$task.id, lrn.ind = seq_along(lrn.list)) runs = lapply(seq_row(grid), function(i) { message(i) task = getOMLTask(grid$task.id[i]) ind = grid$lrn.ind[i] runTaskMlr(task, lrn.list[[ind]]) }) . And finally upload the runs to OpenML. The upload function (uploadOMLRun) returns the ID of the uploaded run object. When uploading runs that are part of a certain study, tag it with study_ and the study ID. After uploading the runs appear on the website and can be found using the tag or via the study homepage. | . ## please do not spam the OpenML server by uploading these ## tasks. I already did that. run.id = lapply(runs, uploadOMLRun, tags = &quot;study_27&quot;) . To show the results of our study, list the run evaluations and make a nice plot. | . evals = listOMLRunEvaluations(tag = &quot;study_27&quot;) evals$task.id = as.factor(evals$task.id) evals$setup.id = as.factor(evals$setup.id) library(&quot;ggplot2&quot;) ggplot(evals, aes(x = setup.id, y = predictive.accuracy, color = data.name, group = task.id)) + geom_point() + geom_line() . . Now you can go ahead and create a bigger study using the techniques you have learned. . Further infos . If you are interested in more, check out the OpenML blog, the paper and the GitHub repos. . . Originally published at mlr-org.github.io. .",
            "url": "https://openml.github.io/blog/openml/mlr/r/2016/09/18/mlr-loves-OpenML.html",
            "relUrl": "/openml/mlr/r/2016/09/18/mlr-loves-OpenML.html",
            "date": " • Sep 18, 2016"
        }
        
    
  
    
        ,"post6": {
            "title": "OpenML",
            "content": "OpenML is a very cool new online platform that aims at improving — as the name says — Open Machine Learning. It stands for Open Data, Open Algorithms and Open Research. OpenML is still in it’s beta phase, but already pretty awesome. . With this blog post I would like to introduce the main concepts, show who should be interested in the platform and I will go a little into a challenge it faces. . Concepts . The following four concepts form the basis of the platform: . data . | task . | flow . | run . | . The figure shows how they are connected. . . Who can make use of OpenML? . The domain scientist . You have data that you do not know how to analyse best? Upload your data to OpenML and you will have the whole world helping you. Write a good data and task description to make sure people understand the problem. . The data analyst . You like taking part in challenges? Being the best solver of a task? Go to OpenML and check out the many tasks and go solve! . The algorithm developer . You developed a statistical method or a machine learning algorithm and want to try it out? You will find plenty of data sets and the possibility to make your algorithm public. . The student . You study statistics, data science, machine learning? You want to know what is out there? On OpenML you will find a wide variety of algorithms and, if the solvers do a good job, info on software and implementation. . The teacher . You teach a machine learning class and want the students to participate in a challenge? Make up your own task and let the students try solving it. The platform shows who uploaded what and when. . The unknown . There are possibly many other people who will benefit from the platform, like meta analysts, benchmarkers and people I can not think of right now. . How to use OpenML . Other than just browsing the website you can access OpenML through quite some interfaces such as R or WEKA. For an example on how to use the R interface check out the tutorial. . The whole project is of course open source. Check out the different git repositories for all the code and in case you have any complaints. . The overfitting problem . Platforms like kaggle, crowdanalytics and innocentive host challenges and give people only part of the data so they can evaluate the performance of the algorithm on a separate data set to (try to) prevent overfitting. So far OpenML does not do that. It always shows all the data, and algorithms are evaluated via resampling procedures (on OpenML called estimation procedures). There are big discussions about how to solve the problem of overfitting on OpenML. They go from keeping part of the data hidden for a certain time in the beginning to doing repeated cross-validation on the (overly) good performing flows on a given task. If you have ideas here, please don’t hesitate to leave me a comment. . The platform is still in it’s childhood and may not be perfect yet (If you find issues, post them on the github page). But I think it can grow to be a great thing one day. . . Originally published at heidiseibold.github.io on May 2, 2016. .",
            "url": "https://openml.github.io/blog/openml/2016/05/02/OpeML.html",
            "relUrl": "/openml/2016/05/02/OpeML.html",
            "date": " • May 2, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About OpenML",
          "content": "OpenML is an open source platform where you can host, review, share, and organize machine learning datasets, algorithms, and experiments through clean and open interfaces. We are a group of machine learning researchers and enthousiasts who believe in making machine learning more streamlined, accessible, and beneficial to all of humanity. .",
          "url": "https://openml.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://openml.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}